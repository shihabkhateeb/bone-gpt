{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "Xfc6__EtjgEV"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd \n",
        "from datasets import load_dataset\n",
        "from datasets import Dataset\n",
        "from transformers import AutoTokenizer, DataCollatorWithPadding, TrainingArguments, AutoModelForSequenceClassification, Trainer\n",
        "import random\n",
        "import torch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "d-R9B0bdlOrj"
      },
      "outputs": [],
      "source": [
        "def tokenize_function(example):\n",
        "    return (example)\n",
        "\n",
        "df = pd.read_csv(\"erraticana-offtopic-raw.csv\")\n",
        "df = df[df[\"Author\"] == \"bonepriest#6318\"]\n",
        "bone_dataset = df[df[\"Content\"].str.strip().str.contains(r\"\\s\", regex=True, na=False) == True]\n",
        "bone_dataset = bone_dataset.drop(['AuthorID', 'Author', 'Date', 'Attachments', 'Reactions'], axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "Jx5Eqx5WyjbU"
      },
      "outputs": [],
      "source": [
        "bone_content = bone_dataset.Content.values.tolist()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "zukjLTnJsVB7"
      },
      "outputs": [],
      "source": [
        "random.shuffle(bone_content)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "661lvcc1sfD1"
      },
      "outputs": [],
      "source": [
        "split_train_valid = 0.9\n",
        "\n",
        "# split dataset\n",
        "train_size = int(split_train_valid * len(bone_dataset))\n",
        "valid_size = len(bone_dataset) - train_size\n",
        "train_dataset, valid_dataset = torch.utils.data.random_split(bone_content, [train_size, valid_size])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "Ypr0KWRBmue0"
      },
      "outputs": [],
      "source": [
        "def make_dataset(dataset, epochs):\n",
        "    total_text = '<|endoftext|>'\n",
        "    message = [t for t in dataset]\n",
        "    for _ in range(epochs):\n",
        "        random.shuffle(message)\n",
        "        total_text += '<|endoftext|>'.join(message) + '<|endoftext|>'\n",
        "    return total_text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mi-NhaT6tHkh",
        "outputId": "ff4a71d0-d5b1-48a1-c1db-358d4634a14c"
      },
      "outputs": [],
      "source": [
        "EPOCHS = 4\n",
        "\n",
        "with open('bone_train.txt', 'w', encoding='utf-8') as f:\n",
        "    data = make_dataset(train_dataset, EPOCHS)\n",
        "    f.write(data)\n",
        "\n",
        "with open('bone_valid.txt', 'w', encoding='utf-8') as f:\n",
        "    data = make_dataset(valid_dataset, 1)\n",
        "    f.write(data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 876
        },
        "id": "kHDEQ4kLtqEo",
        "outputId": "e2b27173-253e-4c92-831c-a4e1f25ba6c7"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m API key must be 40 characters long, yours was 16\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: C:\\Users\\USER/.netrc\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import wandb\n",
        "wandb.login()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FxQ9RwnQ1mms",
        "outputId": "b1f6c3b1-742a-4d94-e44f-ed2af9cfccf9"
      },
      "outputs": [
        {
          "ename": "CalledProcessError",
          "evalue": "Command '['python', 'run_clm.py', '--output_dir=output/bone-gpt', '--overwrite_output_dir', '--overwrite_cache', '--model_type=gpt2', '--model_name_or_path=gpt2', '--do_train', '--train_file=bone_train.txt', '--do_eval', '--validation_file=bone_valid.txt', '--eval_steps=20', '--logging_steps=20', '--per_gpu_train_batch_size=1', '--num_train_epochs=1']' returned non-zero exit status 1.",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mCalledProcessError\u001b[0m                        Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[16], line 25\u001b[0m\n\u001b[0;32m      5\u001b[0m valid_file_path \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mbone_valid.txt\u001b[39m\u001b[39m\"\u001b[39m  \n\u001b[0;32m      7\u001b[0m command \u001b[39m=\u001b[39m [\n\u001b[0;32m      8\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mpython\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m      9\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mrun_clm.py\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     22\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39m--num_train_epochs=1\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m     23\u001b[0m ]\n\u001b[1;32m---> 25\u001b[0m subprocess\u001b[39m.\u001b[39;49mrun(command, check\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[0;32m     27\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m     28\u001b[0m     result \u001b[39m=\u001b[39m subprocess\u001b[39m.\u001b[39mrun(command, check\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, capture_output\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, text\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n",
            "File \u001b[1;32mc:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\subprocess.py:571\u001b[0m, in \u001b[0;36mrun\u001b[1;34m(input, capture_output, timeout, check, *popenargs, **kwargs)\u001b[0m\n\u001b[0;32m    569\u001b[0m     retcode \u001b[39m=\u001b[39m process\u001b[39m.\u001b[39mpoll()\n\u001b[0;32m    570\u001b[0m     \u001b[39mif\u001b[39;00m check \u001b[39mand\u001b[39;00m retcode:\n\u001b[1;32m--> 571\u001b[0m         \u001b[39mraise\u001b[39;00m CalledProcessError(retcode, process\u001b[39m.\u001b[39margs,\n\u001b[0;32m    572\u001b[0m                                  output\u001b[39m=\u001b[39mstdout, stderr\u001b[39m=\u001b[39mstderr)\n\u001b[0;32m    573\u001b[0m \u001b[39mreturn\u001b[39;00m CompletedProcess(process\u001b[39m.\u001b[39margs, retcode, stdout, stderr)\n",
            "\u001b[1;31mCalledProcessError\u001b[0m: Command '['python', 'run_clm.py', '--output_dir=output/bone-gpt', '--overwrite_output_dir', '--overwrite_cache', '--model_type=gpt2', '--model_name_or_path=gpt2', '--do_train', '--train_file=bone_train.txt', '--do_eval', '--validation_file=bone_valid.txt', '--eval_steps=20', '--logging_steps=20', '--per_gpu_train_batch_size=1', '--num_train_epochs=1']' returned non-zero exit status 1."
          ]
        }
      ],
      "source": [
        "import os\n",
        "import subprocess\n",
        "\n",
        "train_file_path = \"bone_train.txt\"\n",
        "valid_file_path = \"bone_valid.txt\"  \n",
        "\n",
        "command = [\n",
        "    \"python\",\n",
        "    \"run_clm.py\",\n",
        "    \"--output_dir=output/bone-gpt\",\n",
        "    \"--overwrite_output_dir\",\n",
        "    \"--overwrite_cache\",\n",
        "    \"--model_type=gpt2\",\n",
        "    \"--model_name_or_path=gpt2\",\n",
        "    \"--do_train\",\n",
        "    f\"--train_file={train_file_path}\",\n",
        "    \"--do_eval\",\n",
        "    f\"--validation_file={valid_file_path}\",\n",
        "    \"--eval_steps=20\",\n",
        "    \"--logging_steps=20\",\n",
        "    \"--per_gpu_train_batch_size=1\",\n",
        "    \"--num_train_epochs=1\"\n",
        "]\n",
        "\n",
        "subprocess.run(command, check=True)\n",
        "\n",
        "try:\n",
        "    result = subprocess.run(command, check=True, capture_output=True, text=True)\n",
        "    print(result.stdout)  # Output from the command\n",
        "except subprocess.CalledProcessError as e:\n",
        "    print(e.stderr)  # Error output from the command\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PQJed3l9yy0J",
        "outputId": "e3629eaa-666c-44e0-9a21-563201787b28"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "env: WANDB_PROJECT=bonegpt-dev\n"
          ]
        }
      ],
      "source": [
        "%env WANDB_PROJECT=bonegpt-dev"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8wxK879I3LFi"
      },
      "outputs": [],
      "source": [
        "SENTENCES = [\"I think that\",\n",
        "             \"I like\",\n",
        "             \"I don't like\",\n",
        "             \"I want\",\n",
        "             \"My dream is\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "psvlcT1A3Lte"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "seed = random.randint(0, 2**32-1)\n",
        "seed"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wEWm9Uof3Q3x"
      },
      "outputs": [],
      "source": [
        "examples = []\n",
        "num_return_sequences = 5\n",
        "\n",
        "for start in SENTENCES:\n",
        "    val = !python run_generation.py \\\n",
        "        --model_type gpt2 \\\n",
        "        --model_name_or_path output/bone_gpt \\\n",
        "        --length 160 \\\n",
        "        --num_return_sequences $num_return_sequences \\\n",
        "        --temperature 1 \\\n",
        "        --p 0.95 \\\n",
        "        --seed $seed \\\n",
        "        --prompt {'\"<|endoftext|>' + start + '\"'}\n",
        "    generated = [val[-1-2*k] for k in range(num_return_sequences)[::-1]]\n",
        "    print(f'\\nStart of sentence: {start}')\n",
        "    for i, g in enumerate(generated):\n",
        "        g = g.replace('<|endoftext|>', '')\n",
        "        print(f'* Generated #{i+1}: {g}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
